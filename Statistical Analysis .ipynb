{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOvGzhrw9IONl0b4A9bXR+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gEdKjlakgExb"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from scipy.integrate import odeint\n","\n","# Define the system of ODEs\n","def system_ode(y, t, b, D1, D2, D3, Beta, Alpha, Mu, Eta):\n","    S, I1, I2, R = y\n","    dS_dt = b - (Beta * S * I1) - (D1 * S)\n","    dI1_dt = (Beta * S * I1) - ((D1 + D2 + Alpha) * I1) + (Eta * I1)\n","    dI2_dt = (Alpha * I1) - ((D2 + D3 + D3 + Mu) * I2)\n","    dR_dt = (Mu * I2) - ((D1 + D2) * R)\n","    return [dS_dt, dI1_dt, dI2_dt, dR_dt]\n","\n","# Generate synthetic data\n","def generate_data(params, initial_conditions, t):\n","    return odeint(system_ode, initial_conditions, t, args=params)\n","\n","# Normalize data\n","def normalize(data, data_min=None, data_max=None):\n","    if data_min is None or data_max is None:\n","        data_min, data_max = np.min(data, axis=0), np.max(data, axis=0)\n","    return (data - data_min) / (data_max - data_min), data_min, data_max\n","\n","# Denormalize data\n","def denormalize(data, data_min, data_max):\n","    return data * (data_max - data_min) + data_min\n","\n","# Define Swish ReLU activation function\n","def swish_relu(x):\n","    return x * tf.nn.sigmoid(x) + tf.nn.relu(x)  # Swish + ReLU\n","Parameters and initial conditions\n","params = (0.03, 0.01, 0.03, 0.03, 0.01, 0.095, 0.90, 0.03)\n","initial_conditions = [100, 5, 1, 0.0]\n","t = np.linspace(0, 50, 500)\n","\n","# Generate data and normalize\n","synthetic_data = generate_data(params, initial_conditions, t)\n","synthetic_data, y_min, y_max = normalize(synthetic_data)\n","t, t_min, t_max = normalize(t.reshape(-1, 1))\n","\n","# Convert to tensors\n","X_train = tf.convert_to_tensor(t, dtype=tf.float32)\n","Y_train = tf.convert_to_tensor(synthetic_data, dtype=tf.float32)\n","\n","# Training function with early stopping\n","def train_pinn(model, X_train, Y_train, epochs=5000, learning_rate=1e-5, patience=500):\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    loss_history = []\n","    best_loss, wait = float(\"inf\"), 0  # Early stopping criteria\n","\n","    for epoch in range(epochs):\n","        with tf.GradientTape() as tape:\n","            predictions = model(X_train)\n","            loss = tf.reduce_mean(tf.square(predictions - Y_train))  # MSE Loss\n","\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        loss_history.append(loss.numpy())\n","\n","        # Print progress every 100 epochs\n","        if epoch % 100 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n","\n","        # Early stopping condition\n","        if loss.numpy() < best_loss:\n","            best_loss = loss.numpy()\n","            wait = 0\n","        else:\n","            wait += 1\n","            if wait > patience:\n","                print(f\"Early stopping at epoch {epoch}\")\n","                break\n","\n","    return loss_history\n","\n","# Mean Squared Error (MSE)\n","def mean_squared_error(actual, predicted):\n","    return np.mean(np.square(actual - predicted))\n","\n","# Run 20 trials and store MSE values for each compartment\n","num_trials = 20\n","mse_values_S, mse_values_I1, mse_values_I2, mse_values_R = [], [], [], []\n","\n","for trial in range(num_trials):\n","    print(f\"\\n=== Trial {trial + 1}/{num_trials} ===\")\n","    model = PINN()\n","    train_pinn(model, X_train, Y_train, epochs=20000, learning_rate=1e-5)  # Early stopping inside\n","\n","    predictions = model(X_train).numpy()\n","    predictions_denorm = denormalize(predictions, y_min, y_max)\n","    synthetic_data_denorm = denormalize(Y_train.numpy(), y_min, y_max)\n","\n","    # Compute MSE values\n","    mse_values_S.append(mean_squared_error(synthetic_data_denorm[:, 0], predictions_denorm[:, 0]))\n","    mse_values_I1.append(mean_squared_error(synthetic_data_denorm[:, 1], predictions_denorm[:, 1]))\n","    mse_values_I2.append(mean_squared_error(synthetic_data_denorm[:, 2], predictions_denorm[:, 2]))\n","    mse_values_R.append(mean_squared_error(synthetic_data_denorm[:, 3], predictions_denorm[:, 3]))\n","\n","# Create a DataFrame for MSE values\n","mse_results = {\n","    \"S (Susceptible)\": mse_values_S,\n","    \"I1 (Infected 1)\": mse_values_I1,\n","    \"I2 (Infected 2)\": mse_values_I2,\n","    \"R (Recovered)\": mse_values_R\n","}\n","\n","mse_df = pd.DataFrame(mse_results, index=[f\"Trial {i+1}\" for i in range(num_trials)])\n","\n","# Print MSE values table\n","print(\"\\n=== MSE Values for Each Compartment Across Trials ===\")\n","print(mse_df.to_string())\n","\n","# Plot MSE convergence for each compartment\n","plt.figure(figsize=(12, 8))\n","plt.plot(range(1, num_trials + 1), mse_values_S, marker='o', linestyle='--', label='S (Susceptible)', color='b')\n","plt.plot(range(1, num_trials + 1), mse_values_I1, marker='o', linestyle='--', label='I1 (Infected 1)', color='r')\n","plt.plot(range(1, num_trials + 1), mse_values_I2, marker='o', linestyle='--', label='I2 (Infected 2)', color='g')\n","plt.plot(range(1, num_trials + 1), mse_values_R, marker='o', linestyle='--', label='R (Recovered)', color='purple')\n","\n","plt.xlabel(\"Trial Number\")\n","plt.ylabel(\"Mean Squared Error (MSE)\")\n","plt.title(\"MSE Convergence Across Trials for Each Compartment\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from scipy.integrate import odeint\n","\n","# Define the system of ODEs\n","def system_ode(y, t, b, D1, D2, D3, Beta, Alpha, Mu, Eta):\n","    S, I1, I2, R = y\n","    dS_dt = b - (Beta * S * I1) - (D1 * S)\n","    dI1_dt = (Beta * S * I1) - ((D1 + D2 + Alpha) * I1) + (Eta * I1)\n","    dI2_dt = (Alpha * I1) - ((D2 + D3 + D3 + Mu) * I2)\n","    dR_dt = (Mu * I2) - ((D1 + D2) * R)\n","    return [dS_dt, dI1_dt, dI2_dt, dR_dt]\n","\n","# Generate synthetic data\n","def generate_data(params, initial_conditions, t):\n","    return odeint(system_ode, initial_conditions, t, args=params)\n","\n","# Normalize data\n","def normalize(data, data_min=None, data_max=None):\n","    if data_min is None or data_max is None:\n","        data_min, data_max = np.min(data, axis=0), np.max(data, axis=0)\n","    return (data - data_min) / (data_max - data_min), data_min, data_max\n","\n","# Denormalize data\n","def denormalize(data, data_min, data_max):\n","    return data * (data_max - data_min) + data_min\n","\n","# Define Swish ReLU activation function\n","def swish_relu(x):\n","    return x * tf.nn.sigmoid(x) + tf.nn.relu(x)  # Swish + ReLU"],"metadata":{"id":"Rs4lS-nKiejS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parameters and initial conditions\n","params = (0.03, 0.01, 0.03, 0.03, 0.01, 0.095, 0.90, 0.03)\n","initial_conditions = [100, 5, 1, 0.0]\n","t = np.linspace(0, 50, 500)\n","\n","# Generate data and normalize\n","synthetic_data = generate_data(params, initial_conditions, t)\n","synthetic_data, y_min, y_max = normalize(synthetic_data)\n","t, t_min, t_max = normalize(t.reshape(-1, 1))\n","\n","# Convert to tensors\n","X_train = tf.convert_to_tensor(t, dtype=tf.float32)\n","Y_train = tf.convert_to_tensor(synthetic_data, dtype=tf.float32)\n","\n","# Training function with early stopping\n","def train_pinn(model, X_train, Y_train, epochs=5000, learning_rate=1e-5, patience=500):\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    loss_history = []\n","    best_loss, wait = float(\"inf\"), 0  # Early stopping criteria\n","\n","    for epoch in range(epochs):\n","        with tf.GradientTape() as tape:\n","            predictions = model(X_train)\n","            loss = tf.reduce_mean(tf.abs(predictions - Y_train))  # MAD Loss\n","\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        loss_history.append(loss.numpy())\n","\n","        # Print progress every 100 epochs\n","        if epoch % 100 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n","\n","        # Early stopping condition\n","        if loss.numpy() < best_loss:\n","            best_loss = loss.numpy()\n","            wait = 0\n","        else:\n","            wait += 1\n","            if wait > patience:\n","                print(f\"Early stopping at epoch {epoch}\")\n","                break\n","\n","    return loss_history\n","\n","# Mean Absolute Deviation (MAD)\n","def mean_absolute_deviation(actual, predicted):\n","    return np.mean(np.abs(actual - predicted))\n","\n","# Run 20 trials and store MAD values for each compartment\n","num_trials = 20\n","mad_values_S, mad_values_I1, mad_values_I2, mad_values_R = [], [], [], []\n","\n","for trial in range(num_trials):\n","    print(f\"\\n=== Trial {trial + 1}/{num_trials} ===\")\n","    model = PINN()\n","    train_pinn(model, X_train, Y_train, epochs=20000, learning_rate=1e-5)  # Early stopping inside\n","\n","    predictions = model(X_train).numpy()\n","    predictions_denorm = denormalize(predictions, y_min, y_max)\n","    synthetic_data_denorm = denormalize(Y_train.numpy(), y_min, y_max)\n","\n","    # Compute MAD values\n","    mad_values_S.append(mean_absolute_deviation(synthetic_data_denorm[:, 0], predictions_denorm[:, 0]))\n","    mad_values_I1.append(mean_absolute_deviation(synthetic_data_denorm[:, 1], predictions_denorm[:, 1]))\n","    mad_values_I2.append(mean_absolute_deviation(synthetic_data_denorm[:, 2], predictions_denorm[:, 2]))\n","    mad_values_R.append(mean_absolute_deviation(synthetic_data_denorm[:, 3], predictions_denorm[:, 3]))\n","\n","# Create a DataFrame for MAD values\n","mad_results = {\n","    \"S (Susceptible)\": mad_values_S,\n","    \"I1 (Infected 1)\": mad_values_I1,\n","    \"I2 (Infected 2)\": mad_values_I2,\n","    \"R (Recovered)\": mad_values_R\n","}\n","\n","mad_df = pd.DataFrame(mad_results, index=[f\"Trial {i+1}\" for i in range(num_trials)])\n","\n","# Print MAD values table\n","print(\"\\n=== MAD Values for Each Compartment Across Trials ===\")\n","print(mad_df.to_string())\n","\n","# Plot MAD convergence for each compartment\n","plt.figure(figsize=(12, 8))\n","plt.plot(range(1, num_trials + 1), mad_values_S, marker='o', linestyle='--', label='S (Susceptible)', color='b')\n","plt.plot(range(1, num_trials + 1), mad_values_I1, marker='o', linestyle='--', label='I1 (Infected 1)', color='r')\n","plt.plot(range(1, num_trials + 1), mad_values_I2, marker='o', linestyle='--', label='I2 (Infected 2)', color='g')\n","plt.plot(range(1, num_trials + 1), mad_values_R, marker='o', linestyle='--', label='R (Recovered)', color='purple')\n","\n","plt.xlabel(\"Trial Number\")\n","plt.ylabel(\"Mean Absolute Deviation (MAD)\")\n","plt.title(\"MAD Convergence Across Trials for Each Compartment\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"emYN71k6isX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from scipy.integrate import odeint\n","\n","# Define the system of ODEs\n","def system_ode(y, t, b, D1, D2, D3, Beta, Alpha, Mu, Eta):\n","    S, I1, I2, R = y\n","    dS_dt = b - (Beta * S * I1) - (D1 * S)\n","    dI1_dt = (Beta * S * I1) - ((D1 + D2 + Alpha) * I1) + (Eta * I1)\n","    dI2_dt = (Alpha * I1) - ((D2 + D3 + D3 + Mu) * I2)\n","    dR_dt = (Mu * I2) - ((D1 + D2) * R)\n","    return [dS_dt, dI1_dt, dI2_dt, dR_dt]\n","\n","# Generate synthetic data\n","def generate_data(params, initial_conditions, t):\n","    return odeint(system_ode, initial_conditions, t, args=params)\n","\n","# Normalize data\n","def normalize(data, data_min=None, data_max=None):\n","    if data_min is None or data_max is None:\n","        data_min, data_max = np.min(data, axis=0), np.max(data, axis=0)\n","    return (data - data_min) / (data_max - data_min), data_min, data_max\n","\n","# Denormalize data\n","def denormalize(data, data_min, data_max):\n","    return data * (data_max - data_min) + data_min\n","\n","# Define Swish ReLU activation function\n","def swish_relu(x):\n","    return x * tf.nn.sigmoid(x) + tf.nn.relu(x)\n","\n","# Parameters and initial conditions\n","params = (0.03, 0.01, 0.03, 0.03, 0.01, 0.095, 0.90, 0.03)\n","initial_conditions = [100, 5, 1, 0.0]\n","t = np.linspace(0, 50, 500)\n","\n","# Generate data and normalize\n","synthetic_data = generate_data(params, initial_conditions, t)\n","synthetic_data, y_min, y_max = normalize(synthetic_data)\n","t, t_min, t_max = normalize(t.reshape(-1, 1))\n","\n","# Convert to tensors\n","X_train = tf.convert_to_tensor(t, dtype=tf.float32)\n","Y_train = tf.convert_to_tensor(synthetic_data, dtype=tf.float32)\n","\n","# Training function with early stopping\n","def train_pinn(model, X_train, Y_train, epochs=5000, learning_rate=1e-5, patience=500):\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    loss_history = []\n","    best_loss, wait = float(\"inf\"), 0\n","\n","    for epoch in range(epochs):\n","        with tf.GradientTape() as tape:\n","            predictions = model(X_train)\n","            loss = tf.reduce_mean(tf.abs(predictions - Y_train))  # Using MAD loss\n","\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        loss_history.append(loss.numpy())\n","\n","        if epoch % 100 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n","\n","        # Early stopping\n","        if loss.numpy() < best_loss:\n","            best_loss = loss.numpy()\n","            wait = 0\n","        else:\n","            wait += 1\n","            if wait > patience:\n","                print(f\"Early stopping at epoch {epoch}\")\n","                break\n","\n","    return loss_history\n","\n","# Theil's Inequality Coefficient (TIC)\n","def theil_inequality_coefficient(actual, predicted):\n","    num = np.sqrt(np.mean((actual - predicted) ** 2))  # Root Mean Square Error\n","    denom = np.sqrt(np.mean(actual ** 2)) + np.sqrt(np.mean(predicted ** 2))\n","    return num / denom if denom != 0 else np.nan\n","\n","# Run 20 trials and store TIC values\n","num_trials = 20\n","tic_values_S, tic_values_I1, tic_values_I2, tic_values_R = [], [], [], []\n","\n","for trial in range(num_trials):\n","    print(f\"\\n=== Trial {trial + 1}/{num_trials} ===\")\n","    model = PINN()\n","    train_pinn(model, X_train, Y_train, epochs=20000, learning_rate=1e-5)\n","\n","    predictions = model(X_train).numpy()\n","    predictions_denorm = denormalize(predictions, y_min, y_max)\n","    synthetic_data_denorm = denormalize(Y_train.numpy(), y_min, y_max)\n","\n","    # Compute Theil's Inequality Coefficient (TIC)\n","    tic_values_S.append(theil_inequality_coefficient(synthetic_data_denorm[:, 0], predictions_denorm[:, 0]))\n","    tic_values_I1.append(theil_inequality_coefficient(synthetic_data_denorm[:, 1], predictions_denorm[:, 1]))\n","    tic_values_I2.append(theil_inequality_coefficient(synthetic_data_denorm[:, 2], predictions_denorm[:, 2]))\n","    tic_values_R.append(theil_inequality_coefficient(synthetic_data_denorm[:, 3], predictions_denorm[:, 3]))\n","\n","# Create a DataFrame for TIC values\n","tic_results = {\n","    \"S (Susceptible)\": tic_values_S,\n","    \"I1 (Infected 1)\": tic_values_I1,\n","    \"I2 (Infected 2)\": tic_values_I2,\n","    \"R (Recovered)\": tic_values_R\n","}\n","\n","tic_df = pd.DataFrame(tic_results, index=[f\"Trial {i+1}\" for i in range(num_trials)])\n","\n","# Print TIC values table\n","print(\"\\n=== Theil's Inequality Coefficient for Each Compartment Across Trials ===\")\n","print(tic_df.to_string())\n","\n","# Plot TIC convergence for each compartment\n","plt.figure(figsize=(12, 8))\n","plt.plot(range(1, num_trials + 1), tic_values_S, marker='o', linestyle='--', label='S (Susceptible)', color='b')\n","plt.plot(range(1, num_trials + 1), tic_values_I1, marker='o', linestyle='--', label='I1 (Infected 1)', color='r')\n","plt.plot(range(1, num_trials + 1), tic_values_I2, marker='o', linestyle='--', label='I2 (Infected 2)', color='g')\n","plt.plot(range(1, num_trials + 1), tic_values_R, marker='o', linestyle='--', label='R (Recovered)', color='purple')\n","\n","plt.xlabel(\"Trial Number\")\n","plt.ylabel(\"Theil's Inequality Coefficient (TIC)\")\n","plt.title(\"TIC Convergence Across Trials for Each Compartment\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"AAQ33aICi2nS"},"execution_count":null,"outputs":[]}]}